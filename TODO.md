## åˆ†ææŸå€‹ conversation (æ­¤åŠŸèƒ½å·²å®Œæˆ)

```bash
./target/debug/codex_usage analysis --path examples/test_conversation.jsonl
./target/debug/codex_usage analysis --path examples/test_conversation.jsonl --output examples/analysis_claude_code.json
./target/debug/codex_usage analysis --path examples/test_conversation_oai.jsonl
./target/debug/codex_usage analysis --path examples/test_conversation_oai.jsonl --output examples/analysis_codex.json
```

## æŸ¥çœ‹ç‰ˆæœ¬è³‡è¨Š
```bash
./target/debug/codex_usage version
# ğŸš€ Coding CLI Helper
#
# â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
# â”‚                                    â”‚
# â”‚  Version:    5.0.6                 â”‚
# â”‚  Rust Version: 1.28.2              â”‚
# â”‚  Cargo Version: 1.89.0             â”‚
# â”‚                                    â”‚
# â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
#
./target/debug/codex_usage version --json
# {
#     "Version": "5.0.6",
#     "Rust Version": "1.28.2",
#     "Cargo Version": "1.89.0"
# }
./target/debug/codex_usage version --text
# Version: 5.0.6
# Rust Version: 1.28.2
# Cargo Version: 1.89.0
```

## æŸ¥çœ‹ä½¿ç”¨ç‹€æ³
```bash
./target/debug/codex_usage update
# å…ˆä¸ç”¨å®Œæˆ å¿½ç•¥
./target/debug/codex_usage usage
# ç›®å‰åŠŸèƒ½æ­£ç¢º ä½†è«‹é€é `Ratatui` ç¾åŒ–è¼¸å‡ºçš„ Table
./target/debug/codex_usage usage --json
# ç›®å‰åŠŸèƒ½æ­£ç¢º å¿½ç•¥
./target/debug/codex_usage help
# ç›®å‰åŠŸèƒ½æ­£ç¢º å¿½ç•¥
```

## æ›´æ–° Usage Table é¡¯ç¤ºå…§å®¹

é€™è£¡æœ‰æ‰€æœ‰æ¨¡å‹çš„åƒ¹æ ¼ `https://github.com/BerriAI/litellm/raw/refs/heads/main/model_prices_and_context_window.json`
ä»–çš„æ ¼å¼å¤§æ¦‚æ˜¯é€™æ¨£çš„
```json
{
    "gpt-5": {
        "cache_read_input_token_cost": 1.25e-07,
        "cache_read_input_token_cost_flex": 6.25e-08,
        "cache_read_input_token_cost_priority": 2.5e-07,
        "input_cost_per_token": 1.25e-06,
        "input_cost_per_token_flex": 6.25e-07,
        "input_cost_per_token_priority": 2.5e-06,
        "litellm_provider": "openai",
        "max_input_tokens": 272000,
        "max_output_tokens": 128000,
        "max_tokens": 128000,
        "mode": "chat",
        "output_cost_per_token": 1e-05,
        "output_cost_per_token_flex": 5e-06,
        "output_cost_per_token_priority": 2e-05,
        "supported_endpoints": [
            "/v1/chat/completions",
            "/v1/batch",
            "/v1/responses"
        ],
        "supported_modalities": [
            "text",
            "image"
        ],
        "supported_output_modalities": [
            "text"
        ],
        "supports_function_calling": true,
        "supports_native_streaming": true,
        "supports_parallel_function_calling": true,
        "supports_pdf_input": true,
        "supports_prompt_caching": true,
        "supports_reasoning": true,
        "supports_response_schema": true,
        "supports_system_messages": true,
        "supports_tool_choice": true,
        "supports_vision": true
    }
}
```
æˆ‘å¸Œæœ›è¨ˆç®—usage çš„æ™‚å€™ å¯ä»¥å…ˆå¾é€™è£¡å–å¾—åƒ¹æ ¼, æœ€å¾Œåšè¨ˆç®—
è€Œä¸æ˜¯å–®ç´”é¡¯ç¤º token ä½¿ç”¨é‡

æˆ‘å¸Œæœ›æ¬„ä½æœ‰ `Date`, `Model`, `Input`, `Output`, `Cache Read`, `Cache Creation`, `Total Tokens` å’Œ `Cost (USD)`
